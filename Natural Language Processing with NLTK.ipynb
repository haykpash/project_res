{"cells":[{"cell_type":"markdown","source":["December 2, 2024"],"metadata":{"id":"cjjQTxJCadJ_"}},{"cell_type":"markdown","metadata":{"id":"LgLrIkqOjeZm"},"source":["# Natural Language Processing with `nltk`\n","\n","`nltk` is the most popular Python package for Natural Language processing, it provides algorithms for importing, cleaning, pre-processing text data in human language and then apply computational linguistics algorithms like sentiment analysis.\n","\n","It also includes many easy-to-use datasets in the `nltk.corpus` package, we can download for example the `movie_reviews` package using the `nltk.download` function:"]},{"cell_type":"markdown","metadata":{"id":"L2DIQ43GjeZn"},"source":["Inspecting the Movie Reviews Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xqS4pXEjeZn"},"outputs":[],"source":["# Importing the required libraries\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords"]},{"cell_type":"code","source":["# Import the sentiment intensity analyzer\n","!pip install vaderSentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"],"metadata":{"id":"UkSnjr3NAtxV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356861381,"user_tz":480,"elapsed":4510,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"97283ab2-bf04-4780-93ce-a4bba2d9c49b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.12.14)\n","Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPE2kTE0jeZo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356862567,"user_tz":480,"elapsed":1200,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"3d521311-37ff-427e-994e-9ed19f751bcb"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["#Downloading the dataset\n","nltk.download(\"movie_reviews\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cN6fiwBxjeZo"},"outputs":[],"source":["#Running this cell to import the dataset\n","from nltk.corpus import movie_reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFR08MDtjeZo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356862994,"user_tz":480,"elapsed":430,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"38a3f87a-2a55-4f41-e0c1-e7902412f700"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["#Running this cell for later use in tokenization\n","nltk.download('vader_lexicon')  # for sentiment analysis\n","nltk.download('punkt_tab')  # for tokenizing"]},{"cell_type":"markdown","metadata":{"id":"mr7MRewpjeZo"},"source":["Tokenizing Text in Words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LZbdx56jeZo"},"outputs":[],"source":["romeo_text = \"\"\"Why then, O brawling love! O loving hate!\n","O any thing, of nothing first create!\n","O heavy lightness, serious vanity,\n","Misshapen chaos of well-seeming forms,\n","Feather of lead, bright smoke, cold fire, sick health,\n","Still-waking sleep, that is not what it is!\n","This love feel I, that feel no love in this.\"\"\""]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"NOYVwETgjeZo"},"source":["The first step in Natural Language processing is generally to split the text into words, this process might appear simple but it is very tedious to handle all corner cases, see for example all the issues with punctuation we have to solve if we just start with a split on whitespace.\n","\n","**Splitting `romeo_text` by spaces and storing the resultant list of words in the variable `romeo_tokens`**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-5gZFidFjeZo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356862995,"user_tz":480,"elapsed":5,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"665248fc-7570-48a8-d766-3d120d2fbad3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Why',\n"," 'then,',\n"," 'O',\n"," 'brawling',\n"," 'love!',\n"," 'O',\n"," 'loving',\n"," 'hate!',\n"," 'O',\n"," 'any',\n"," 'thing,',\n"," 'of',\n"," 'nothing',\n"," 'first',\n"," 'create!']"]},"metadata":{},"execution_count":8}],"source":["# Split the romeo_text into a list of words and store it in romeo_tokens\n","romeo_tokens = romeo_text.split()\n","romeo_tokens[:15]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"-UAgp_gBjeZq"},"outputs":[],"source":["assert type(romeo_tokens) == list\n","assert len(romeo_tokens) == 52"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"2T0E5MRLjeZq"},"source":["`nltk` has a sophisticated word tokenizer trained on English named `punkt` which we imported earlier in the notebook."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"IzelcAqQjeZq"},"source":["**Using the `nltk.word_tokenize(text)` function to properly tokenize `romeo_text` and stores the result as `romeo_words`."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ww0hQiEljeZq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356863222,"user_tz":480,"elapsed":230,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"22b012c5-7e22-4aa5-8374-cd757bc07f00"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Why',\n"," 'then',\n"," ',',\n"," 'O',\n"," 'brawling',\n"," 'love',\n"," '!',\n"," 'O',\n"," 'loving',\n"," 'hate',\n"," '!',\n"," 'O',\n"," 'any',\n"," 'thing',\n"," ',']"]},"metadata":{},"execution_count":10}],"source":["# Use word_tokenize to properly tokenize romeo_text into individual words and punctuation\n","romeo_words = word_tokenize(romeo_text)\n","romeo_words[:15]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"b8uUSX-HjeZq"},"outputs":[],"source":["assert type(romeo_words) == list\n","assert len(romeo_words) == 68"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"jJ7L2t6sjeZq"},"source":["## Building a bag-of-words model\n","\n","The simplest model for analyzing text is just to think about text as an unordered collection of words (bag-of-words). This can generally allow to infer from the text the category, the topic or the sentiment.\n","\n","From the bag-of-words model we can build features to be used by a classifier, here we assume that each word is a feature that can either be `True` or `False`.\n","We implement this in Python as a dictionary where for each word in a sentence we associate `True`.\n","\n","**Writting a function `build_bag_of_words(words)` which returns a dictionary with {word : True} formatting given a set of words. Calling the function with `romeo_words` and storing the resultant dictionary as `romeo_word_dict`.**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"d3xHS-l5jeZq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356863222,"user_tz":480,"elapsed":9,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"a561c67c-f9b0-4688-b899-d5df61d891c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Why': True,\n"," 'then': True,\n"," ',': True,\n"," 'O': True,\n"," 'brawling': True,\n"," 'love': True,\n"," '!': True,\n"," 'loving': True,\n"," 'hate': True,\n"," 'any': True,\n"," 'thing': True,\n"," 'of': True,\n"," 'nothing': True,\n"," 'first': True,\n"," 'create': True,\n"," 'heavy': True,\n"," 'lightness': True,\n"," 'serious': True,\n"," 'vanity': True,\n"," 'Misshapen': True,\n"," 'chaos': True,\n"," 'well-seeming': True,\n"," 'forms': True,\n"," 'Feather': True,\n"," 'lead': True,\n"," 'bright': True,\n"," 'smoke': True,\n"," 'cold': True,\n"," 'fire': True,\n"," 'sick': True,\n"," 'health': True,\n"," 'Still-waking': True,\n"," 'sleep': True,\n"," 'that': True,\n"," 'is': True,\n"," 'not': True,\n"," 'what': True,\n"," 'it': True,\n"," 'This': True,\n"," 'feel': True,\n"," 'I': True,\n"," 'no': True,\n"," 'in': True,\n"," 'this': True,\n"," '.': True}"]},"metadata":{},"execution_count":12}],"source":["# Creatting a bag-of-words dictionary where each word is a key with a value of True\n","def build_bag_of_words_features(words):\n","    return {word: True for word in words}\n","\n","# Generating the bag-of-words dictionary for romeo_words\n","romeo_word_dict = build_bag_of_words_features(romeo_words)\n","romeo_word_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"xUqOjgjDjeZq"},"outputs":[],"source":["# Sanity check\n","assert type(build_bag_of_words_features(romeo_words)) == dict\n","assert sum(value for value in romeo_word_dict.values() if value) == 45"]},{"cell_type":"markdown","metadata":{"id":"yfjjBI50jeZq"},"source":["This is what we wanted, but we notice that also punctuation like \"!\" and words useless for classification purposes like \"of\" or \"that\" are also included.\n","Those words are named \"stopwords\" and `nltk` has a convenient corpus we can download:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOQS0TKQjeZq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356863222,"user_tz":480,"elapsed":7,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"bb2ec67a-5d7f-4dc7-9c78-f10771cd70b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}],"source":["nltk.download(\"stopwords\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNXMb6AnjeZr","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1735356863222,"user_tz":480,"elapsed":6,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"c11ee814-1128-44fd-d55e-3006e015a1aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["import string\n","string.punctuation"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"_lPMqep_jeZr"},"source":["Using the Python `string.punctuation` list and the English stopwords we can build better features by filtering out those words that would not help in the classification.\n","\n","**Creatting a list `useless_words` that is a collection of stopwords in english and the punctuation characters.**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Zpq6sCA9jeZr"},"outputs":[],"source":["# Create a list of useless words combining English stopwords and punctuation characters\n","useless_words = list(set(stopwords.words('english')).union(set(string.punctuation)))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Yt5CQ8amjeZr"},"outputs":[],"source":["assert type(useless_words) == list\n","assert len(useless_words) == 211"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"oro_XnIJjeZr"},"source":["**Writing a function `build_bag_of_words_features_filtered(words)` that returns a filtered bag of words - a dictionary with only useful words as key and 1 as the value. Calling this function with `romeo_words` and storing the resultant dictionary as `romeo_useful_word_dict`.**"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"6m4Ob8mXjeZr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356863223,"user_tz":480,"elapsed":6,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"40fa97b2-b481-426e-bdda-1d3ffb8303c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Why': True,\n"," 'O': True,\n"," 'brawling': True,\n"," 'love': True,\n"," 'loving': True,\n"," 'hate': True,\n"," 'thing': True,\n"," 'nothing': True,\n"," 'first': True,\n"," 'create': True,\n"," 'heavy': True,\n"," 'lightness': True,\n"," 'serious': True,\n"," 'vanity': True,\n"," 'Misshapen': True,\n"," 'chaos': True,\n"," 'well-seeming': True,\n"," 'forms': True,\n"," 'Feather': True,\n"," 'lead': True,\n"," 'bright': True,\n"," 'smoke': True,\n"," 'cold': True,\n"," 'fire': True,\n"," 'sick': True,\n"," 'health': True,\n"," 'Still-waking': True,\n"," 'sleep': True,\n"," 'This': True,\n"," 'feel': True,\n"," 'I': True}"]},"metadata":{},"execution_count":18}],"source":["# Creatting a filtered bag-of-words dictionary excluding useless words (stopwords and punctuation)\n","def build_bag_of_words_features_filtered(words):\n","    return {word: True for word in words if word not in useless_words}\n","\n","# Generating the filtered bag-of-words dictionary for romeo_words\n","romeo_useful_word_dict = build_bag_of_words_features_filtered(romeo_words)\n","romeo_useful_word_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"PaJxLlwMjeZr"},"outputs":[],"source":["# Sanity check\n","assert type(build_bag_of_words_features_filtered(romeo_words)) == dict\n","assert len(romeo_useful_word_dict) == 31"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Jgs6tClFjeZr"},"source":["## Frequencies of Words\n","\n","It is common to explore a dataset before starting the analysis, in this section we will find the most common words and plot their frequency.\n","\n","Using the `movie_reviews.words()` (the nltk corpus we imported previously) with no argument we can extract the words from the entire dataset as `all_words` and check that it is about 1.6 millions."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"OP--a4nqjeZr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356871946,"user_tz":480,"elapsed":8728,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"eb24932d-3450-4f13-d1eb-193673039cbc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['plot',\n"," ':',\n"," 'two',\n"," 'teen',\n"," 'couples',\n"," 'go',\n"," 'to',\n"," 'a',\n"," 'church',\n"," 'party',\n"," ',',\n"," 'drink',\n"," 'and',\n"," 'then',\n"," 'drive']"]},"metadata":{},"execution_count":20}],"source":["# Extracting all words from the movie_reviews dataset and store them in a list\n","all_words = list(movie_reviews.words())\n","all_words[:15]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"92VCYW3ijeZr"},"source":["Filtering out `useless_words` as defined in the previous section, and create a new list `filtered_words` this will reduce the length of the dataset by more than a factor of 2"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-vMuUmT_jeZr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356878118,"user_tz":480,"elapsed":6174,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"61715041-7388-4621-8048-70f67243bb4d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['plot',\n"," 'two',\n"," 'teen',\n"," 'couples',\n"," 'go',\n"," 'church',\n"," 'party',\n"," 'drink',\n"," 'drive',\n"," 'get',\n"," 'accident',\n"," 'one',\n"," 'guys',\n"," 'dies',\n"," 'girlfriend']"]},"metadata":{},"execution_count":21}],"source":["# Filtering out useless words (stopwords and punctuation) from all_words to create a cleaned list\n","filtered_words = [word for word in all_words if word not in useless_words]\n","filtered_words[:15]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"8OjO8MQ4jeZr"},"outputs":[],"source":["assert type(filtered_words) == list"]},{"cell_type":"markdown","metadata":{"id":"FZ9JvlqdjeZw"},"source":["The `collection` package of the standard library contains a `Counter` class that is handy for counting frequencies of words in our list:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8Oy7kKbjeZw"},"outputs":[],"source":["from collections import Counter\n","word_counter = Counter(filtered_words)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"f47yTmyMjeZw"},"source":["It also has a [most_common() ](https://pythontic.com/containers/counter/most_common) method of the word_counter and store the top 10 used words from the corpus in `most_common_words`."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"7YS84BJtjeZw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356878394,"user_tz":480,"elapsed":279,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"2be8b154-6028-4cec-d8e2-e340e3041689"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('film', 9517),\n"," ('one', 5852),\n"," ('movie', 5771),\n"," ('like', 3690),\n"," ('even', 2565),\n"," ('good', 2411),\n"," ('time', 2411),\n"," ('story', 2169),\n"," ('would', 2109),\n"," ('much', 2049)]"]},"metadata":{},"execution_count":24}],"source":["# Retrieving the top 10 most common words and their frequencies from the word_counter\n","most_common_words = word_counter.most_common(10)\n","most_common_words"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"6pkTYJeFjeZw"},"outputs":[],"source":["assert type(most_common_words) == list\n","assert len(most_common_words) == 10"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"kCLf4jiFjeZw"},"source":["\n","## Sentiment Analysis\n","\n","Using the sentiment intensity analyzer, loop over the `list_sentences` and print the polarity scores of each of the sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"xY8I4gNIjeZw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735356878394,"user_tz":480,"elapsed":5,"user":{"displayName":"Michael Newman","userId":"08685120094313325931"}},"outputId":"c831a681-7776-4bc6-9131-0b25c47881a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: Hello, how are you?\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","Sentiment: Neutral\n","\n","Sentence: Today is a nice day\n","Scores: {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.4215}\n","Sentiment: Positive\n","\n","Sentence: I don't like the food at the cafe\n","Scores: {'neg': 0.232, 'neu': 0.768, 'pos': 0.0, 'compound': -0.2755}\n","Sentiment: Negative\n","\n","Sentence: This is the worst pizza I have ever had.\n","Scores: {'neg': 0.339, 'neu': 0.661, 'pos': 0.0, 'compound': -0.6249}\n","Sentiment: Negative\n","\n","Sentence: The orange juice is delicious!\n","Scores: {'neg': 0.0, 'neu': 0.501, 'pos': 0.499, 'compound': 0.6114}\n","Sentiment: Positive\n","\n","Sentence: I am late to class.\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","Sentiment: Neutral\n","\n"]}],"source":["# Analyze the sentiment of each sentence in the list and print the polarity scores and sentiment\n","analyzer = SentimentIntensityAnalyzer()\n","\n","list_sentences = [\"Hello, how are you?\", \"Today is a nice day\", \"I don't like the food at the cafe\", \"This is the worst pizza I have ever had.\", \"The orange juice is delicious!\", \"I am late to class.\" ]\n","\n","def calculate_sentiment(sentence):\n","    scores = analyzer.polarity_scores(sentence)\n","    sentiment = (\n","        \"Positive\" if scores[\"compound\"] >= 0.05\n","        else \"Negative\" if scores[\"compound\"] <= -0.05\n","        else \"Neutral\"\n","    )\n","    return scores, sentiment\n","\n","# Loop through sentences, calculate sentiment, and print results\n","for sentence in list_sentences:\n","    scores, sentiment = calculate_sentiment(sentence)\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Scores: {scores}\")\n","    print(f\"Sentiment: {sentiment}\\n\")"]}],"metadata":{"kernelspec":{"display_name":"ta_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"otter":{"OK_FORMAT":true,"tests":{"q1":{"name":"q1","points":0.5,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q2":{"name":"q2","points":0.5,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q3":{"name":"q3","points":1,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q4":{"name":"q4","points":0.5,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q5":{"name":"q5","points":1,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q6":{"name":"q6","points":0.5,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q7":{"name":"q7","points":0.5,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q8":{"name":"q8","points":0.5,"suites":[{"cases":[],"scored":true,"setup":"","teardown":"","type":"doctest"}]}}},"colab":{"provenance":[{"file_id":"1jY_kR3KAdxerlpxXVV07IsGcNpVrW6Xz","timestamp":1732908635580}]}},"nbformat":4,"nbformat_minor":0}